{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Requirements Files"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%writefile SETTINGS.json\n","{\n","    \"DATA_BASE_DIR\": \"../input/\",\n","    \"MAX_SEQ_LENGTH\": 1536,\n","    \"USE_FP16\": true,\n","    \"NUM_CORES\": 16,\n","    \"BATCH_SIZE\": 4,\n","    \"GRAD_ACCUM_STEPS\": 2,\n","    \"NUM_EPOCHS\": 5,\n","    \"LR\": 2e-5\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile requirements.txt\n","torch==1.7.0\n","transformers==4.17.0\n","tensorboard==2.6.0\n","pandas==1.4.1\n","numpy==1.21.2\n","scipy==1.6.3"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%writefile trainer.py\n","import os\n","import json\n","import pickle\n","import argparse\n","import numpy as np\n","import pandas as pd\n","import transformers\n","from functools import partial\n","import multiprocessing as mp\n","from scipy.special import softmax\n","from torch.utils.data import Dataset\n","from transformers import (AutoModelForTokenClassification,\n","                          DataCollatorForTokenClassification, \n","                          TrainingArguments, \n","                          Trainer,\n","                          TrainerCallback)\n","\n","ap = argparse.ArgumentParser()\n","ap.add_argument(\"--fold\", type=int)\n","ap.add_argument(\"--pretrained_model\", type=str)\n","cargs = ap.parse_args()\n","\n","if (\"deberta-v3\" in cargs.pretrained_model)|(\"deberta-v2\" in cargs.pretrained_model):\n","    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast as AutoTokenizer\n","else:\n","    from transformers import AutoTokenizer\n","\n","cfg = json.load(open('SETTINGS.json', 'r'))\n","\n","DATA_BASE_DIR = cfg[\"DATA_BASE_DIR\"]\n","LR = cfg[\"LR\"]\n","NUM_EPOCHS = cfg[\"NUM_EPOCHS\"]\n","NUM_CORES = cfg[\"NUM_CORES\"]\n","BATCH_SIZE = cfg[\"BATCH_SIZE\"]\n","USE_FP16 = cfg[\"USE_FP16\"]\n","GRAD_ACCUM_STEPS = cfg[\"GRAD_ACCUM_STEPS\"]\n","MAX_SEQ_LENGTH = cfg[\"MAX_SEQ_LENGTH\"]\n","SPLIT_NUM = cargs.fold\n","PRETRAINED_MODEL = cargs.pretrained_model\n","\n","TRAIN_CSV = os.path.join(DATA_BASE_DIR, 'feedback-prize-2021/train.csv')\n","TRAIN_DIR = os.path.join(DATA_BASE_DIR, 'feedback-prize-2021/train/')\n","\n","MIN_TOKENS = {\n","    \"Lead\": 6,\n","    \"Position\": 3,\n","    \"Evidence\": 20,\n","    \"Claim\": 1,\n","    \"Concluding Statement\": 3,\n","    \"Counterclaim\": 7,\n","    \"Rebuttal\": 6\n","}\n","\n","train_df = pd.read_csv(TRAIN_CSV)\n","train_df['discourse_id'] = train_df['discourse_id'].astype('long').astype('str')\n","train_df['discourse_start'] = train_df['discourse_start'].astype('int')\n","train_df['discourse_end'] = train_df['discourse_end'].astype('int')\n","\n","folds = pickle.load(open(os.path.join(DATA_BASE_DIR, 'feedbackgroupshufflesplit1337/groupshufflesplit_1337.p'), 'rb'))\n","\n","ner_labels = ['O']\n","for curr_label in train_df['discourse_type'].unique():\n","    ner_labels.append('B-' + curr_label)\n","    ner_labels.append('I-' + curr_label)\n","ner_labels = dict((x,i) for i,x in enumerate(ner_labels))\n","\n","inverted_ner_labels = dict((v,k) for k,v in ner_labels.items())\n","inverted_ner_labels[-100] = 'Special Token'\n","\n","\n","def calc_overlap(row):\n","    \"\"\"\n","    Calculates the overlap between prediction and\n","    ground truth and overlap percentages used for determining\n","    true positives.\n","    \"\"\"\n","    set_pred = set(row.predictionstring_pred.split(' '))\n","    set_gt = set(row.predictionstring_gt.split(' '))\n","    # Length of each and intersection\n","    len_gt = len(set_gt)\n","    len_pred = len(set_pred)\n","    inter = len(set_gt.intersection(set_pred))\n","    overlap_1 = inter / len_gt\n","    overlap_2 = inter/ len_pred\n","    return [overlap_1, overlap_2]\n","\n","def score_feedback_comp(pred_df, gt_df):\n","    \"\"\"\n","    A function that scores for the kaggle\n","        Student Writing Competition\n","        \n","    Uses the steps in the evaluation page here:\n","        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n","    \"\"\"\n","    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n","        .reset_index(drop=True).copy()\n","    pred_df = pred_df[['id','class','predictionstring']] \\\n","        .reset_index(drop=True).copy()\n","    pred_df['pred_id'] = pred_df.index\n","    gt_df['gt_id'] = gt_df.index\n","    # Step 1. all ground truths and predictions for a given class are compared.\n","    joined = pred_df.merge(gt_df,\n","                           left_on=['id','class'],\n","                           right_on=['id','discourse_type'],\n","                           how='outer',\n","                           suffixes=('_pred','_gt')\n","                          )\n","    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n","    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n","\n","    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n","\n","    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n","    # and the overlap between the prediction and the ground truth >= 0.5,\n","    # the prediction is a match and considered a true positive.\n","    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n","    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n","    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n","    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n","    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n","    tp_pred_ids = joined.query('potential_TP') \\\n","        .sort_values('max_overlap', ascending=False) \\\n","        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n","\n","    # 3. Any unmatched ground truths are false negatives\n","    # and any unmatched predictions are false positives.\n","    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n","\n","    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n","    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n","\n","    # Get numbers of each type\n","    TP = len(tp_pred_ids)\n","    FP = len(fp_pred_ids)\n","    FN = len(unmatched_gt_ids)\n","    #calc microf1\n","    my_f1_score = TP / (TP + 0.5*(FP+FN))\n","    return my_f1_score\n","\n","def generate_token_to_word_mapping(txt, offset):\n","    # GET WORD POSITIONS IN CHARS\n","    w = []\n","    blank = True\n","    for i in range(len(txt)):\n","        if not txt[i].isspace() and blank==True:\n","            w.append(i)\n","            blank=False\n","        elif txt[i].isspace():\n","            blank=True\n","    w.append(1e6)\n","\n","    # MAPPING FROM TOKENS TO WORDS\n","    word_map = -1 * np.ones(len(offset),dtype='int32')\n","    w_i = 0\n","    for i in range(len(offset)):\n","        if offset[i][1]==0: continue\n","        while offset[i][0]>=w[w_i+1]: w_i += 1\n","        word_map[i] = int(w_i)\n","        \n","    return word_map\n","\n","class TextOverlapFBetaScore:\n","    def __init__(self, test_df, test_dataset):\n","        self.test_df = test_df\n","        self.test_dataset = test_dataset\n","\n","    def __call__(self, raw_predictions):\n","        predictions, _ = raw_predictions\n","        soft_predictions = softmax(predictions, -1)\n","        soft_predictions = np.max(soft_predictions, axis=-1)\n","        predictions = np.argmax(predictions, axis=-1)\n","\n","        all_preds = []\n","        # Clumsy gathering of predictions at word lvl - only populate with 1st subword pred\n","        for curr_sample_id in range(len(self.test_dataset)):\n","            sample_preds = predictions[curr_sample_id]\n","            sample_offset = self.test_dataset.get_offset(curr_sample_id)\n","            sample_txt = ner_valid_rows[curr_sample_id][1]\n","            sample_word_map = generate_token_to_word_mapping(sample_txt, sample_offset)\n","\n","            word_preds = [''] * (max(sample_word_map) + 1)\n","            word_probs = dict()\n","            for i, curr_word_id in enumerate(sample_word_map):\n","                if curr_word_id != -1 and word_preds[curr_word_id] == '': # only use 1st subword\n","                    word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n","                    word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n","\n","            # Dict to hold Lead, Position, Concluding Statement\n","            let_one_dict = dict() # K = Type, V = (Prob of start token, start, end)\n","\n","            # If we see tokens I-X, I-Y, I-X in a sequence -> change I-Y to I-X\n","            for j in range(1, len(word_preds) - 1):\n","                pred_trio = [word_preds[k] for k in [j - 1, j, j + 1]]\n","                splitted_trio = [x.split('-')[0] for x in pred_trio]\n","                if all([x == 'I' for x in splitted_trio]) and pred_trio[0] == pred_trio[2] and pred_trio[0] != pred_trio[1]:\n","                    word_preds[j] = word_preds[j-1]\n","\n","            j = 0 # start of candidate discourse\n","            while j < len(word_preds): \n","                cls = word_preds[j] \n","                cls_splitted = cls.split('-')[-1]\n","                end = j + 1 # try to extend discourse as far as possible\n","\n","                if word_probs[j] > 0.63: \n","                    # Must match suffix i.e., I- to I- only; no B- to I-\n","                    while end < len(word_preds) and (word_preds[end].split('-')[-1] == cls_splitted if cls_splitted in ['Lead', 'Position', 'Concluding Statement'] else word_preds[end] == f'I-{cls_splitted}'):\n","                        end += 1\n","                    # if we're here, end is not the same pred as start\n","                    if cls != 'O' and end - j > MIN_TOKENS[cls_splitted]: # needs to be longer than class-specified min\n","                        if cls_splitted in ['Lead', 'Position', 'Concluding Statement']:\n","                            lpc_max_prob = max(word_probs[c] for c in range(j, end))\n","                            if cls_splitted in let_one_dict: # Already existing, check contiguous or higher prob\n","                                prev_prob, prev_start, prev_end = let_one_dict[cls_splitted]\n","                                if j - prev_end < 3: # If close enough, combine\n","                                    let_one_dict[cls_splitted] = (max(prev_prob, lpc_max_prob), prev_start, end)\n","                                elif lpc_max_prob > prev_prob: # Overwrite if current candidate is more likely\n","                                    let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n","                            else: # Add to it\n","                                let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n","                        else:\n","                            # Lookback and add preceding I- tokens\n","                            while j - 1 > 0 and word_preds[j-1] == cls:\n","                                j = j - 1\n","                            # Try to add the matching B- tag if immediately precedes the current I- sequence\n","                            if j - 1 > 0 and word_preds[j-1] == f'B-{cls_splitted}':\n","                                j = j - 1\n","\n","                            all_preds.append((self.test_dataset.get_filename(curr_sample_id), \n","                                              cls_splitted, \n","                                              ' '.join(map(str, list(range(j, end+1))))))\n","\n","                j = end \n","\n","            # Add the Lead, Position, Concluding Statement\n","            for k, v in let_one_dict.items():\n","                pred_start = v[1]\n","                # Lookback and add preceding I- tokens\n","                while pred_start - 1 > 0 and word_preds[pred_start-1] == f'I-{k}':\n","                    pred_start = pred_start -1\n","                # Try to add the matching B- tag if immediately precedes the current I- sequence\n","                if pred_start - 1 > 0 and word_preds[pred_start - 1] == f'B-{k}':\n","                    pred_start = pred_start - 1\n","\n","                all_preds.append((self.test_dataset.get_filename(curr_sample_id), \n","                                  k, \n","                                  ' '.join(map(str, list(range(pred_start, v[2]))))))\n","\n","        output_df = pd.DataFrame(all_preds)\n","        output_df.columns = ['id', 'class', 'predictionstring']\n","\n","        f1s = []\n","        CLASSES = output_df['class'].unique()\n","        for c in CLASSES:\n","            pred_df = output_df.loc[output_df['class']==c].copy()\n","            gt_df = self.test_df.loc[self.test_df['discourse_type']==c].copy()\n","            f1 = score_feedback_comp(pred_df, gt_df)\n","            f1s.append(f1)\n","\n","        return {\"textoverlapfbeta\": np.mean(f1s)}\n","\n","class SaveBestModelCallback(TrainerCallback):\n","    def __init__(self):\n","        self.bestScore = 0\n","\n","    def on_train_begin(self, args, state, control, **kwargs):\n","        assert args.evaluation_strategy != \"no\", \"SaveBestModelCallback requires IntervalStrategy of steps or epoch\"\n","\n","    def on_evaluate(self, args, state, control, metrics, **kwargs):\n","        metric_value = metrics.get(\"eval_textoverlapfbeta\")\n","        if metric_value > self.bestScore:\n","            print(f\"** TextOverlapFBeta score improved from {np.round(self.bestScore, 4)} to {np.round(metric_value, 4)} **\")\n","            self.bestScore = metric_value\n","            control.should_save = True\n","        else:\n","            print(f\"TextOverlapFBeta score {np.round(metric_value, 4)} (Prev. Best {np.round(self.bestScore, 4)}) \")\n","\n","valid_df = train_df.iloc[folds[SPLIT_NUM][1]].reset_index(drop=True)\n","train_df = train_df.iloc[folds[SPLIT_NUM][0]].reset_index(drop=True)\n","\n","train_files = train_df['id'].unique()\n","valid_files = valid_df['id'].unique()\n","\n","# accepts file path, returns tuple of (file_ID, txt, NER labels)\n","def generate_NER_labels_for_file(input_filename, df):\n","    curr_id = input_filename.split('.')[0]\n","    with open(os.path.join(TRAIN_DIR, '{}.txt'.format(input_filename))) as f:\n","        curr_txt = f.read()\n","\n","    # Set all token labels initially to non-label\n","    curr_labels = [ner_labels['O']] * len(curr_txt)\n","    # Iterate thru all labels associated w/ file and update labels\n","    curr_df = df[df['id']==curr_id]\n","    for curr_discourse in curr_df.itertuples():\n","        curr_discourse_label = curr_discourse.discourse_type \n","        for curr_txt_idx in range(curr_discourse.discourse_start, \n","                                  min(curr_discourse.discourse_end+1, len(curr_labels))):\n","            if curr_txt_idx == curr_discourse.discourse_start:\n","                iob_label = ner_labels['B-' + curr_discourse_label]\n","            else:\n","                iob_label = ner_labels['I-' + curr_discourse_label]\n","            curr_labels[curr_txt_idx] = iob_label\n","    assert curr_labels != [ner_labels['O']] * len(curr_txt)\n","    return curr_id, curr_txt, curr_labels\n","\n","with mp.Pool(NUM_CORES) as p:\n","    ner_train_rows = p.map(partial(generate_NER_labels_for_file, df=train_df), train_files)\n","\n","with mp.Pool(NUM_CORES) as p:\n","    ner_valid_rows = p.map(partial(generate_NER_labels_for_file, df=valid_df), valid_files)\n","\n","tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n","\n","# Check is rust-based fast tokenizer\n","assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n","\n","def tokenize_and_align_labels(ner_raw_data):\n","    tokenized_inputs = tokenizer([x[1] for x in ner_raw_data], \n","                                 max_length=MAX_SEQ_LENGTH,\n","                                 return_offsets_mapping=True,\n","                                 truncation=True)\n","    labels = []\n","    word_ids = []\n","    for i, char_label in enumerate([x[2] for x in ner_raw_data]):\n","        curr_word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        curr_offset_mappings = tokenized_inputs['offset_mapping'][i]\n","        label_ids = []\n","        for j, word_idx in enumerate(curr_word_ids):\n","            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n","            # ignored in the loss function.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            else:\n","                # Use label of 1st character of word\n","                # See offset that's 1 index after end of tokens for some reason\n","                char_idx = min(curr_offset_mappings[j][0], len(char_label)-1)\n","                label_ids.append(char_label[char_idx])\n","        word_ids.append(curr_word_ids)\n","        labels.append(label_ids)\n","\n","    tokenized_inputs['labels'] = labels\n","    tokenized_inputs['word_ids'] = word_ids\n","    tokenized_inputs['id'] = [x[0] for x in ner_raw_data]\n","    return tokenized_inputs\n","\n","tokenized_all_train = tokenize_and_align_labels(ner_train_rows)\n","tokenized_all_valid = tokenize_and_align_labels(ner_valid_rows)\n","\n","class NERDataset(Dataset):\n","    def __init__(self, input_dict):\n","        self.input_dict = input_dict\n","        \n","    def __getitem__(self, index):\n","        return {k:self.input_dict[k][index] for k in self.input_dict.keys() if k not in {'id', 'offset_mapping', 'word_ids'}}\n","    \n","    def get_filename(self, index):\n","        return self.input_dict['id'][index]\n","    \n","    def get_offset(self, index):\n","        return self.input_dict['offset_mapping'][index]\n","    \n","    def __len__(self):\n","        return len(self.input_dict['input_ids'])\n","\n","train_dataset = NERDataset(tokenized_all_train)\n","valid_dataset = NERDataset(tokenized_all_valid)\n","\n","model = AutoModelForTokenClassification.from_pretrained(PRETRAINED_MODEL, \n","                                                        num_labels=len(ner_labels),\n","                                                        trust_remote_code=True)\n","\n","model_name = PRETRAINED_MODEL.split('/')[-1]\n","args = TrainingArguments(f'{model_name}-{SPLIT_NUM}',\n","                         PRETRAINED_MODEL,\n","                         lr_scheduler_type='linear',\n","                         evaluation_strategy = 'steps',\n","                         eval_steps=500,\n","                         dataloader_num_workers=8,\n","                         learning_rate=LR,\n","                         log_level='warning',\n","                         fp16 = USE_FP16,\n","                         per_device_train_batch_size=BATCH_SIZE,\n","                         per_device_eval_batch_size=1,\n","                         gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n","                         gradient_checkpointing=True,\n","                         num_train_epochs=NUM_EPOCHS,\n","                         save_strategy='no',\n","                         save_total_limit=1)\n","\n","data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","trainer = Trainer(model,\n","                  args=args,\n","                  train_dataset=train_dataset,\n","                  eval_dataset=valid_dataset,\n","                  compute_metrics=TextOverlapFBetaScore(test_df=valid_df, test_dataset=valid_dataset),\n","                  callbacks=[SaveBestModelCallback],\n","                  data_collator=data_collator,\n","                  tokenizer=tokenizer)\n","\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 0 --pretrained_model microsoft/deberta-large"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 1 --pretrained_model microsoft/deberta-large"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 2 --pretrained_model microsoft/deberta-large"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 3 --pretrained_model microsoft/deberta-large"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 0 --pretrained_model microsoft/deberta-xlarge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 1 --pretrained_model microsoft/deberta-xlarge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 2 --pretrained_model microsoft/deberta-xlarge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 3 --pretrained_model microsoft/deberta-xlarge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 4 --pretrained_model microsoft/deberta-xlarge"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["! python trainer.py --fold 5 --pretrained_model microsoft/deberta-xlarge"]},{"cell_type":"markdown","metadata":{},"source":["# Inference Script with Post Process"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["%%writefile generate_preds.py\n","\n","import os\n","import argparse\n","\n","ap = argparse.ArgumentParser()\n","ap.add_argument('--model_paths', nargs='+', required=True)\n","ap.add_argument(\"--save_name\", type=str, required=True)\n","ap.add_argument(\"--max_len\", type=int, required=True)\n","args = ap.parse_args()\n","\n","        \n","import gc\n","import pickle\n","import numpy as np\n","import pandas as pd\n","import transformers\n","import multiprocessing as mp\n","from scipy.special import softmax\n","from torch.utils.data import Dataset\n","from transformers import (AutoModelForTokenClassification, \n","                          AutoTokenizer, \n","                          TrainingArguments, \n","                          Trainer)\n","\n","os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n","\n","NUM_CORES = 16\n","BATCH_SIZE = 4\n","MAX_SEQ_LENGTH = args.max_len\n","PRETRAINED_MODEL_PATHS = args.model_paths\n","if \"debertal_chris\" in args.save_name:\n","    print('==> using -1 in offset mapping...')\n","    \n","AGG_FUNC = np.mean\n","print('==> using span token mean...')\n","\n","TEST_DIR = '../input/feedback-prize-2021/test/'\n","\n","MIN_TOKENS = {\n","    \"Lead\": 32,\n","    \"Position\": 5,\n","    \"Evidence\": 35,\n","    \"Claim\": 7,\n","    \"Concluding Statement\": 6,\n","    \"Counterclaim\": 6,\n","    \"Rebuttal\": 6\n","}\n","\n","if \"chris\" not in args.save_name:\n","    ner_labels = {'O': 0,\n","                  'B-Lead': 1,\n","                  'I-Lead': 2,\n","                  'B-Position': 3,\n","                  'I-Position': 4,\n","                  'B-Evidence': 5,\n","                  'I-Evidence': 6,\n","                  'B-Claim': 7,\n","                  'I-Claim': 8,\n","                  'B-Concluding Statement': 9,\n","                  'I-Concluding Statement': 10,\n","                  'B-Counterclaim': 11,\n","                  'I-Counterclaim': 12,\n","                  'B-Rebuttal': 13,\n","                  'I-Rebuttal': 14}\n","else:\n","    print(\"==> Using Chris BIO\")\n","    ner_labels = {'O': 14,\n","                  'B-Lead': 0,\n","                  'I-Lead': 1,\n","                  'B-Position': 2,\n","                  'I-Position': 3,\n","                  'B-Evidence': 4,\n","                  'I-Evidence': 5,\n","                  'B-Claim': 6,\n","                  'I-Claim': 7,\n","                  'B-Concluding Statement': 8,\n","                  'I-Concluding Statement': 9,\n","                  'B-Counterclaim': 10,\n","                  'I-Counterclaim': 11,\n","                  'B-Rebuttal': 12,\n","                  'I-Rebuttal': 13}\n","\n","\n","inverted_ner_labels = dict((v,k) for k,v in ner_labels.items())\n","inverted_ner_labels[-100] = 'Special Token'\n","\n","test_files = os.listdir(TEST_DIR)\n","\n","# accepts file path, returns tuple of (file_ID, txt split, NER labels)\n","def generate_text_for_file(input_filename):\n","    curr_id = input_filename.split('.')[0]\n","    with open(os.path.join(TEST_DIR, input_filename)) as f:\n","        curr_txt = f.read()\n","\n","    return curr_id, curr_txt\n","\n","with mp.Pool(NUM_CORES) as p:\n","    ner_test_rows = p.map(generate_text_for_file, test_files)\n","    \n","\n","tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL_PATHS[0])\n","\n","# Check is rust-based fast tokenizer\n","assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n","\n","ner_test_rows = sorted(ner_test_rows, key=lambda x: len(tokenizer(x[1], max_length=MAX_SEQ_LENGTH, truncation=True)['input_ids']))\n","\n","# tokenize and store word ids\n","def tokenize_with_word_ids(ner_raw_data):\n","    # ner_raw_data is shaped (num_examples, 3) where cols are (ID, words, word-level labels)\n","    tokenized_inputs = tokenizer([x[1] for x in ner_raw_data], \n","                                 max_length=MAX_SEQ_LENGTH,\n","                                 return_offsets_mapping=True,\n","                                 truncation=True)\n","    \n","    tokenized_inputs['id'] = [x[0] for x in ner_raw_data]\n","    tokenized_inputs['offset_mapping'] = [tokenized_inputs['offset_mapping'][i] for i in range(len(ner_raw_data))]\n","    \n","    return tokenized_inputs\n","\n","tokenized_all = tokenize_with_word_ids(ner_test_rows)\n","\n","class NERDataset(Dataset):\n","    def __init__(self, input_dict):\n","        self.input_dict = input_dict\n","        \n","    def __getitem__(self, index):\n","        return {k:self.input_dict[k][index] for k in self.input_dict.keys() if k not in {'id', 'offset_mapping'}}\n","    \n","    def get_filename(self, index):\n","        return self.input_dict['id'][index]\n","    \n","    def get_offset(self, index):\n","        return self.input_dict['offset_mapping'][index]\n","    \n","    def __len__(self):\n","        return len(self.input_dict['input_ids'])\n","\n","test_dataset = NERDataset(tokenized_all)\n","\n","soft_predictions = None\n","hfargs = TrainingArguments(output_dir='None',\n","                         log_level='warning',\n","                         per_device_eval_batch_size=BATCH_SIZE)\n","\n","for idx, curr_path in enumerate(PRETRAINED_MODEL_PATHS):\n","\n","    \n","    model = AutoModelForTokenClassification.from_pretrained(curr_path, trust_remote_code=True)\n","    trainer = Trainer(model,\n","                      hfargs,\n","                      tokenizer=tokenizer)\n","    \n","    curr_preds, _, _ = trainer.predict(test_dataset)\n","    curr_preds = curr_preds.astype(np.float16)\n","    curr_preds = softmax(curr_preds, -1)\n","\n","    if soft_predictions is not None:\n","        soft_predictions = soft_predictions + curr_preds\n","    else:\n","        soft_predictions = curr_preds\n","        \n","    del model, trainer, curr_preds\n","    gc.collect()\n","\n","soft_predictions = soft_predictions / len(PRETRAINED_MODEL_PATHS)\n","\n","soft_claim_predictions = soft_predictions[:, :, 8]\n","\n","predictions = np.argmax(soft_predictions, axis=2)\n","soft_predictions = np.max(soft_predictions, axis=2)\n","\n","def generate_token_to_word_mapping(txt, offset):\n","    # GET WORD POSITIONS IN CHARS\n","    w = []\n","    blank = True\n","    for i in range(len(txt)):\n","        if not txt[i].isspace() and blank==True:\n","            w.append(i)\n","            blank=False\n","        elif txt[i].isspace():\n","            blank=True\n","    w.append(1e6)\n","\n","    # MAPPING FROM TOKENS TO WORDS\n","    word_map = -1 * np.ones(len(offset),dtype='int32')\n","    w_i = 0\n","    for i in range(len(offset)):\n","        if offset[i][1]==0: continue\n","        while offset[i][0]>=(w[w_i+1]-(\"debertal_chris\" in args.save_name)-(\"v3\" in args.save_name)\\\n","                             -(\"v2\" in args.save_name) ): w_i += 1\n","        word_map[i] = int(w_i)\n","\n","    return word_map\n","\n","all_preds = []\n","\n","# Clumsy gathering of predictions at word lvl - only populate with 1st subword pred\n","for curr_sample_id in range(len(test_dataset)):\n","    curr_preds = []\n","    sample_preds = predictions[curr_sample_id]\n","    sample_offset = test_dataset.get_offset(curr_sample_id)\n","    sample_txt = ner_test_rows[curr_sample_id][1]\n","    sample_word_map = generate_token_to_word_mapping(sample_txt, sample_offset)\n","\n","    word_preds = [''] * (max(sample_word_map) + 1)\n","    word_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n","    claim_probs = dict(zip(range((max(sample_word_map) + 1)),[0]*(max(sample_word_map) + 1)))\n","\n","    for i, curr_word_id in enumerate(sample_word_map):\n","        if curr_word_id != -1:\n","            if word_preds[curr_word_id] == '': # only use 1st subword\n","                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n","                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n","                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n","            elif 'B-' in inverted_ner_labels[sample_preds[i]]:\n","                word_preds[curr_word_id] = inverted_ner_labels[sample_preds[i]]\n","                word_probs[curr_word_id] = soft_predictions[curr_sample_id, i]\n","                claim_probs[curr_word_id] = soft_claim_predictions[curr_sample_id, i]\n","\n","    # Dict to hold Lead, Position, Concluding Statement\n","    let_one_dict = dict() # K = Type, V = (Prob of start token, start, end)\n","\n","    # If we see tokens I-X, I-Y, I-X -> change I-Y to I-X\n","    for j in range(1, len(word_preds) - 1):\n","        pred_trio = [word_preds[k] for k in [j - 1, j, j + 1]]\n","        splitted_trio = [x.split('-')[0] for x in pred_trio]\n","        if all([x == 'I' for x in splitted_trio]) and pred_trio[0] == pred_trio[2] and pred_trio[0] != pred_trio[1]:\n","            word_preds[j] = word_preds[j-1]\n","\n","    # B-X, ? (not B), I-X -> change ? to I-X\n","    for j in range(1, len(word_preds) - 1):\n","        if 'B-' in word_preds[j-1] and word_preds[j+1] == f\"I-{word_preds[j-1].split('-')[-1]}\" and word_preds[j] != word_preds[j+1] and 'B-' not in word_preds[j]:\n","            word_preds[j] = word_preds[j+1]\n","\n","     # If we see tokens I-X, O, I-X, change center token to the same for stated discourse types\n","    for j in range(1, len(word_preds) - 1):\n","        if word_preds[j - 1] in ['I-Lead', 'I-Position', 'I-Concluding Statement'] and word_preds[j-1] == word_preds[j+1] and word_preds[j] == 'O':\n","            word_preds[j] = word_preds[j-1]\n","\n","    j = 0 # start of candidate discourse\n","    while j < len(word_preds): \n","        cls = word_preds[j] \n","        cls_splitted = cls.split('-')[-1]\n","        end = j + 1 # try to extend discourse as far as possible\n","\n","        if word_probs[j] > 0.54: \n","            # Must match suffix i.e., I- to I- only; no B- to I-\n","            while end < len(word_preds) and (word_preds[end].split('-')[-1] == cls_splitted if cls_splitted in ['Lead', 'Position', 'Concluding Statement'] else word_preds[end] == f'I-{cls_splitted}'):\n","                end += 1\n","            # if we're here, end is not the same pred as start\n","            if cls != 'O' and (end - j > MIN_TOKENS[cls_splitted] or max(word_probs[l] for l in range(j, end)) > 0.73): # needs to be longer than class-specified min\n","                if cls_splitted in ['Lead', 'Position', 'Concluding Statement']:\n","                    lpc_max_prob = max(word_probs[c] for c in range(j, end))\n","                    if cls_splitted in let_one_dict: # Already existing, check contiguous or higher prob\n","                        prev_prob, prev_start, prev_end = let_one_dict[cls_splitted]\n","                        if cls_splitted in ['Lead', 'Concluding Statement'] and j - prev_end < 49: # If close enough, combine\n","                            let_one_dict[cls_splitted] = (max(prev_prob, lpc_max_prob), prev_start, end)\n","                            \n","                            # Delete other preds that lie inside the joined LC discourse\n","                            for l in range(len(curr_preds) - 1, 0, -1):\n","                                check_span = curr_preds[l][2]\n","                                check_start, check_end = int(check_span[0]), int(check_span[-1])\n","                                if check_start > prev_start and check_end < end:\n","                                    del curr_preds[l]\n","                            \n","                        elif lpc_max_prob > prev_prob: # Overwrite if current candidate is more likely\n","                            let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n","                    else: # Add to it\n","                        let_one_dict[cls_splitted] = (lpc_max_prob, j, end)\n","                else:\n","                    # Lookback and add preceding I- tokens\n","                    while j - 1 > 0 and word_preds[j-1] == cls:\n","                        j = j - 1\n","                    # Try to add the matching B- tag if immediately precedes the current I- sequence\n","                    if j - 1 > 0 and word_preds[j-1] == f'B-{cls_splitted}':\n","                        j = j - 1\n","\n","\n","                    #############################################################\n","                    # Run a bunch of adjustments to discourse predictions based on CV \n","                    adj_start, adj_end = j, end + 1\n","\n","                    # Run some heuristics against previous discourse\n","                    if len(curr_preds) > 0:\n","                        prev_span = list(map(int, curr_preds[-1][2].split()))\n","                        prev_start, prev_end = prev_span[0], prev_span[-1]\n","\n","                        # Join adjacent rebuttals\n","                        if cls_splitted in 'Rebuttal':                        \n","                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 32:\n","                                del curr_preds[-1]\n","                                combined_list = prev_span + list(range(adj_start, adj_end))                                \n","                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \n","                                                   cls_splitted, \n","                                                   ' '.join(map(str, combined_list)),\n","                                                   AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n","                                j = end\n","                                continue\n","                                \n","                        elif cls_splitted in 'Counterclaim':                        \n","                            if curr_preds[-1][1] == cls_splitted and adj_start - prev_end < 24:\n","                                del curr_preds[-1]\n","                                combined_list = prev_span + list(range(adj_start, adj_end))                                \n","                                curr_preds.append((test_dataset.get_filename(curr_sample_id), \n","                                                   cls_splitted, \n","                                                   ' '.join(map(str, combined_list)),\n","                                                  AGG_FUNC([word_probs[i] for i in combined_list if i in word_probs.keys()])))\n","                                j = end\n","                                continue\n","\n","                        elif cls_splitted in 'Evidence':                        \n","                            if curr_preds[-1][1] == cls_splitted and 8 < adj_start - prev_end < 25:\n","                                if max(claim_probs[l] for l in range(prev_end+1, adj_start)) > 0.35:\n","                                    claim_tokens = [str(l) for l in range(prev_end+1, adj_start) if claim_probs[l] > 0.15]\n","                                    if len(claim_tokens) > 2:\n","                                        curr_preds.append((test_dataset.get_filename(curr_sample_id), \n","                                                           'Claim', \n","                                                           ' '.join(claim_tokens),\n","                                                           AGG_FUNC([word_probs[int(i)] for i in claim_tokens if int(i) in word_probs.keys()])))\n","                        # If gap with discourse of same type, extend to it \n","                        elif curr_preds[-1][1] == cls_splitted and adj_start - prev_end > 2:\n","                            adj_start -= 1\n","\n","                    # Adjust discourse lengths if too long or short\n","                    if cls_splitted == 'Evidence':\n","                        if adj_end - adj_start < 45:\n","                            adj_start -= 9\n","                        else:\n","                            adj_end -= 1\n","                    elif cls_splitted == 'Claim':\n","                        if adj_end - adj_start > 24:\n","                            adj_end -= 1\n","                    elif cls_splitted == 'Counterclaim':\n","                        if adj_end - adj_start > 24:\n","                            adj_end -= 1\n","                        else:\n","                            adj_start -= 1\n","                            adj_end += 1\n","                    elif cls_splitted == 'Rebuttal':\n","                        if adj_end - adj_start > 32:\n","                            adj_end -= 1\n","                        else:\n","                            adj_start -= 1\n","                            adj_end += 1\n","                    adj_start = max(0, adj_start)\n","                    adj_end = min(len(word_preds) - 1, adj_end)\n","                    curr_preds.append((test_dataset.get_filename(curr_sample_id), \n","                                       cls_splitted, \n","                                       ' '.join(map(str, list(range(adj_start, adj_end)))),\n","                                       AGG_FUNC([word_probs[i] for i in range(adj_start, adj_end) if i in word_probs.keys()])))\n","\n","        j = end \n","\n","    # Add the Lead, Position, Concluding Statement\n","    for k, v in let_one_dict.items():\n","        pred_start = v[1]\n","        pred_end = v[2]\n","\n","        # Lookback and add preceding I- tokens\n","        while pred_start - 1 > 0 and word_preds[pred_start-1] == f'I-{k}':\n","            pred_start = pred_start - 1\n","        # Try to add the matching B- tag if immediately precedes the current I- sequence\n","        if pred_start - 1 > 0 and word_preds[pred_start - 1] == f'B-{k}':\n","            pred_start = pred_start - 1\n","\n","        # Extend short Leads and Concluding Statements\n","        if k == 'Lead':\n","            if pred_end - pred_start < 33:\n","                pred_end = min(len(word_preds), pred_end + 5)\n","            else:\n","                pred_end -= 5\n","        elif k == 'Concluding Statement':\n","            if pred_end - pred_start < 23:\n","                pred_start = max(0, pred_start - 1)\n","                pred_end = min(len(word_preds), pred_end + 10)\n","        elif k == 'Position':\n","            if pred_end - pred_start < 18:\n","                pred_end = min(len(word_preds), pred_end + 3)\n","\n","        pred_start = max(0, pred_start)\n","        if pred_end - pred_start > 6:\n","            curr_preds.append((test_dataset.get_filename(curr_sample_id), \n","                               k, \n","                               ' '.join(map(str, list(range(pred_start, pred_end)))),\n","                               AGG_FUNC([word_probs[i] for i in range(pred_start, pred_end) if i in word_probs.keys()])))\n","\n","    all_preds.extend(curr_preds)\n","\n","output_df = pd.DataFrame(all_preds)\n","output_df.columns = ['id', 'class', 'predictionstring', 'scores']\n","output_df.to_csv(f'{args.save_name}.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Weighted Box Fusion"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["import warnings\n","import numpy as np\n","\n","def prefilter_boxes(boxes, scores, labels, weights, thr):\n","    # Create dict with boxes stored by its label\n","    new_boxes = dict()\n","\n","    for t in range(len(boxes)):\n","\n","        if len(boxes[t]) != len(scores[t]):\n","            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n","            exit()\n","\n","        for j in range(len(boxes[t])):\n","            score = scores[t][j]\n","            if score < thr:\n","                continue\n","            label = labels[t][j]\n","            box_part = boxes[t][j]\n","\n","            x = float(box_part[0])\n","            y = float(box_part[1])\n","\n","            # Box data checks\n","            if y < x:\n","                warnings.warn('Y < X value in box. Swap them.')\n","                x, y = y, x\n","\n","            # [label, score, weight, model index, x, y]\n","            b = [label, float(score) * weights[t], weights[t], t, x, y]\n","            if label not in new_boxes:\n","                new_boxes[label] = []\n","            new_boxes[label].append(b)\n","\n","    # Sort each list in dict by score and transform it to numpy array\n","    for k in new_boxes:\n","        current_boxes = np.array(new_boxes[k])\n","        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n","\n","    return new_boxes\n","\n","\n","def get_weighted_box(boxes, conf_type='avg'):\n","    \"\"\"\n","    Create weighted box for set of boxes\n","    :param boxes: set of boxes to fuse\n","    :param conf_type: type of confidence one of 'avg' or 'max'\n","    :return: weighted box (label, score, weight, model index, x, y)\n","    \"\"\"\n","\n","    box = np.zeros(6, dtype=np.float32)\n","    conf = 0\n","    conf_list = []\n","    w = 0\n","    for b in boxes:\n","        box[4:] += (b[1] * b[4:])\n","        conf += b[1]\n","        conf_list.append(b[1])\n","        w += b[2]\n","    box[0] = boxes[0][0]\n","    if conf_type == 'avg':\n","        box[1] = conf / len(boxes)\n","    elif conf_type == 'max':\n","        box[1] = np.array(conf_list).max()\n","    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n","        box[1] = conf / len(boxes)\n","    box[2] = w\n","    box[3] = -1 # model index field is retained for consistensy but is not used.\n","    box[4:] /= conf\n","    return box\n","\n","\n","def find_matching_box_quickly(boxes_list, new_box, match_iou):\n","    \"\"\" \n","        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays\n","        (~100x). This was previously the bottleneck since the function is called for every entry in the array.\n","\n","        boxes_list: shape: (N, label, score, weight, model index, x, y)\n","        new_box: shape: (label, score, weight, model index, x, y)\n","    \"\"\"\n","    def bb_iou_array(boxes, new_box):\n","        '''\n","        boxes: shape: (N, x, y)\n","        new_box: shape: (x, y)\n","        '''\n","        # bb interesection over union\n","        x_min = np.minimum(boxes[:, 0], new_box[0])\n","        x_max = np.maximum(boxes[:, 0], new_box[0])\n","        y_min = np.minimum(boxes[:, 1], new_box[1])+1\n","        y_max = np.maximum(boxes[:, 1], new_box[1])+1\n","\n","        iou = np.maximum(0, (y_min-x_max)/(y_max-x_min))\n","\n","        return iou\n","\n","    if boxes_list.shape[0] == 0:\n","        return -1, match_iou\n","\n","    # boxes = np.array(boxes_list)\n","    boxes = boxes_list\n","\n","    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n","\n","    ious[boxes[:, 0] != new_box[0]] = -1\n","\n","    best_idx = np.argmax(ious)\n","    best_iou = ious[best_idx]\n","\n","    if best_iou <= match_iou:\n","        best_iou = match_iou\n","        best_idx = -1\n","\n","    return best_idx, best_iou\n","\n","\n","def weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n","    '''\n","    :param boxes_list: list of boxes predictions from each model, each box is 2 numbers.\n","     It has 3 dimensions (models_number, model_preds, 2)\n","     Order of boxes: x, y.\n","    :param scores_list: list of scores for each model\n","    :param labels_list: list of labels for each model\n","    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model\n","    :param iou_thr: IoU value for boxes to be a match\n","    :param skip_box_thr: exclude boxes with score lower than this variable\n","    :param conf_type: how to calculate confidence in weighted boxes. 'avg': average value, 'max': maximum value, 'box_and_model_avg': box and model wise hybrid weighted average, 'absent_model_aware_avg': weighted average that takes into account the absent model.\n","    :param allows_overflow: false if we want confidence score not exceed 1.0\n","\n","    :return: boxes: boxes coordinates (Order of boxes: x, y).\n","    :return: scores: confidence scores\n","    :return: labels: boxes labels\n","    '''\n","\n","    if weights is None:\n","        weights = np.ones(len(boxes_list))\n","    if len(weights) != len(boxes_list):\n","        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n","        weights = np.ones(len(boxes_list))\n","    weights = np.array(weights)\n","\n","    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n","        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n","        exit()\n","\n","    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n","    if len(filtered_boxes) == 0:\n","        return np.zeros((0, 2)), np.zeros((0,)), np.zeros((0,))\n","\n","    overall_boxes = []\n","    for label in filtered_boxes:\n","        boxes = filtered_boxes[label]\n","        new_boxes = []\n","        weighted_boxes = np.empty((0,6)) ## [label, score, weight, model index, x, y]\n","        # Clusterize boxes\n","        for j in range(0, len(boxes)):\n","            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\n","\n","            if index != -1:\n","                new_boxes[index].append(boxes[j])\n","                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n","            else:\n","                new_boxes.append([boxes[j].copy()])\n","                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n","\n","        # Rescale confidence based on number of models and boxes\n","        for i in range(len(new_boxes)):\n","            clustered_boxes = np.array(new_boxes[i])\n","            if conf_type == 'box_and_model_avg':\n","                # weighted average for boxes\n","                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]\n","                # identify unique model index by model index column\n","                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n","                # rescale by unique model weights\n","                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()\n","            elif conf_type == 'absent_model_aware_avg':\n","                # get unique model index in the cluster\n","                models = np.unique(clustered_boxes[:, 3]).astype(int)\n","                # create a mask to get unused model weights\n","                mask = np.ones(len(weights), dtype=bool)\n","                mask[models] = False\n","                # absent model aware weighted average\n","                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())\n","            elif conf_type == 'max':\n","                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()\n","            elif not allows_overflow:\n","                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()\n","            else:\n","                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()\n","        \n","        # REQUIRE BBOX TO BE PREDICTED BY AT LEAST 2 MODELS\n","        #for i in range(len(new_boxes)):\n","        #    clustered_boxes = np.array(new_boxes[i])\n","        #    if len(np.unique(clustered_boxes[:, 3])) > 1:\n","        #        overall_boxes.append(weighted_boxes[i])\n","                \n","        overall_boxes.append(weighted_boxes) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n","    overall_boxes = np.concatenate(overall_boxes, axis=0) # NOT NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n","    #overall_boxes = np.array(overall_boxes) # NEEDED FOR \"REQUIRE TWO MODELS\" ABOVE\n","    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n","    boxes = overall_boxes[:, 4:]\n","    scores = overall_boxes[:, 1]\n","    labels = overall_boxes[:, 0]\n","    return boxes, scores, labels\n"]},{"cell_type":"markdown","metadata":{},"source":["# Folds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python generate_preds.py --model_paths ../input/deberta-large-100/fold0 \\                                    \n","                            --save_name debertal_f0 --max_len 1536"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python generate_preds.py --model_paths ../input/deberta-large-100/fold0 \\\n","                                        ../input/deberta-large-100/fold1 \\                                    \n","                            --save_name debertal_f1 --max_len 1536"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python generate_preds.py --model_paths ../input/deberta-xlarge-1536/deberta-xlarge-v8004-f4/checkpoint-14000 \\\n","                                                                    --save_name debertaxl-f4 --max_len 1536"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["!python generate_preds.py --model_paths ../input/deberta-large-100/fold0 \\\n","                                        ../input/deberta-large-100/fold1 \\\n","                                        ../input/deberta-large-100/fold2 \\\n","                            --save_name debertal --max_len 1536\n","\n","!python generate_preds.py --model_paths ../input/deberta-xlarge-1536/deberta-xlarge-v8004-f4/checkpoint-14000 \\\n","                                        ../input/deberta-xlarge-1536/deberta-xlarge-v4005-f5/checkpoint-13000 \\\n","                            --save_name debertaxl --max_len 1536"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np, os\n","\n","debertaxl_csv = pd.read_csv(\"debertaxl.csv\").dropna()\n","debertal_csv = pd.read_csv(\"debertal.csv\").dropna()"]},{"cell_type":"markdown","metadata":{},"source":["# Ensemble Models with WBF"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TEST_DIR = '../input/feedback-prize-2021/test/'\n","test_files = os.listdir(TEST_DIR)\n","v_ids = [f.replace('.txt','') for f in test_files]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import math\n","\n","class_to_label = {\n","    'Claim': 0, \n","    'Evidence': 1, \n","    'Lead':2, \n","    'Position':3, \n","    'Concluding Statement':4,\n","    'Counterclaim':5, \n","    'Rebuttal':6\n","}\n","\n","# Threshold found from CV\n","label_to_threshold = {\n","    0 : 0.275, #Claim\n","    1 : 0.375, #Evidence\n","    2 : 0.325, #Lead\n","    3 : 0.325, #Position\n","    4 : 0.4, #Concluding Statement\n","    5 : 0.275, #Counterclaim\n","    6 : 0.275 #Rebuttal\n","}\n","\n","label_to_class = {v:k for k, v in class_to_label.items()}\n","\n","def preprocess_for_wbf(df_list):\n","    boxes_list=[]\n","    scores_list=[]\n","    labels_list=[]\n","    \n","    for df in df_list:\n","        scores_list.append(df['scores'].values.tolist())\n","        labels_list.append(df['class'].map(class_to_label).values.tolist())\n","        predictionstring = df.predictionstring.str.split().values\n","        df_box_list = []\n","        for bb in predictionstring:\n","            df_box_list.append([int(bb[0]), int(bb[-1])])\n","        boxes_list.append(df_box_list)\n","    return boxes_list, scores_list, labels_list\n","\n","def postprocess_for_wbf(idx, boxes_list, scores_list, labels_list):\n","    preds = []\n","    for box, score, label in zip(boxes_list, scores_list, labels_list):\n","        if score > label_to_threshold[label]: \n","            start = math.ceil(box[0])\n","            end = int(box[1])\n","            preds.append((idx, label_to_class[label], ' '.join([str(x) for x in range(start, end+1)])))\n","    return preds\n","\n","def generate_wbf_for_id(i):\n","    df2 = debertal_csv[debertal_csv['id']==i]\n","    df4 = debertaxl_csv[debertaxl_csv['id']==i]\n","    \n","    boxes_list, scores_list, labels_list = preprocess_for_wbf([df2,df4])\n","    nboxes_list, nscores_list, nlabels_list = weighted_boxes_fusion(boxes_list, scores_list, labels_list, iou_thr=0.33, conf_type='avg')\n","\n","    return postprocess_for_wbf(i, nboxes_list, nscores_list, nlabels_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import multiprocessing as mp\n","\n","with mp.Pool(2) as p:\n","    list_of_list = p.map(generate_wbf_for_id, v_ids)\n","\n","preds = [x for sub_list in list_of_list for x in sub_list]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub = pd.DataFrame(preds)\n","sub.columns = [\"id\", \"class\", \"predictionstring\"]\n","sub.to_csv('submission.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualize Test Predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from pathlib import Path\n","from spacy import displacy\n","\n","test_path = Path('../input/feedback-prize-2021/test')\n","\n","colors = {\n","            'Lead': '#8000ff',\n","            'Position': '#2b7ff6',\n","            'Evidence': '#2adddd',\n","            'Claim': '#80ffb4',\n","            'Concluding Statement': 'd4dd80',\n","            'Counterclaim': '#ff8042',\n","            'Rebuttal': '#ff0000',\n","            'Other': '#007f00',\n","         }\n","\n","def get_test_text(ids):\n","    with open(test_path/f'{ids}.txt', 'r') as file: data = file.read()\n","    return data\n","\n","def visualize(df):\n","    ids = df[\"id\"].unique()\n","    for i in range(len(ids)):\n","        ents = []\n","        example = ids[i]\n","        curr_df = df[df[\"id\"]==example]\n","        text = \" \".join(get_test_text(example).split())\n","        splitted_text = text.split()\n","        for i, row in curr_df.iterrows():\n","            predictionstring = row['predictionstring']\n","            predictionstring = predictionstring.split()\n","            wstart = int(predictionstring[0])\n","            wend = int(predictionstring[-1])\n","            ents.append({\n","                             'start': len(\" \".join(splitted_text[:wstart])), \n","                             'end': len(\" \".join(splitted_text[:wend+1])), \n","                             'label': row['class']\n","                        })\n","        ents = sorted(ents, key = lambda i: i['start'])\n","\n","        doc2 = {\n","            \"text\": text,\n","            \"ents\": ents,\n","            \"title\": example\n","        }\n","\n","        options = {\"ents\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal'], \"colors\": colors}\n","        displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if len(sub[\"id\"].unique())==5:\n","    visualize(sub)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":2970755,"sourceId":31779,"sourceType":"competition"},{"datasetId":1968880,"sourceId":3248794,"sourceType":"datasetVersion"},{"datasetId":1984876,"sourceId":3276749,"sourceType":"datasetVersion"}],"dockerImageVersionId":30163,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
